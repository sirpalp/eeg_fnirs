from __future__ import division, print_function
import numpy as np
from keras.models import Model
from keras.layers import Input, LSTM, Dense, TimeDistributed, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D, RepeatVector
from keras.callbacks import ModelCheckpoint, TensorBoard
from keras_radam import RAdam
from contextlib import redirect_stdout

class SequenceToSequenceModel(object):

    def __init__(self, sequence_length, input_dim, output_dim, lstm_units, dense_units):
        self.sequence_length = sequence_length
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.lstm_units = lstm_units
        self.dense_units = dense_units
        self.model = self.build_sequence_to_sequence_model(sequence_length, input_dim, output_dim, lstm_units, dense_units)

    def build_sequence_to_sequence_model(self, sequence_length, input_dim, output_dim, lstm_units, dense_units):
        encoder_inputs = Input(shape=(sequence_length, input_dim), name='encoder_inputs')
        
        # Encoder Layers
        encoder_conv = Conv1D(100, 1, activation='relu')(encoder_inputs)
        encoder_pool = GlobalAveragePooling1D()(encoder_conv)
        encoder_repeat = RepeatVector(1)(encoder_pool)
        encoder_conv1 = Conv1D(1024, 1, activation='tanh')(encoder_repeat)
        encoder_pool1 = GlobalMaxPooling1D()(encoder_conv1)
        encoder_repeat1 = RepeatVector(1)(encoder_pool1)
        encoder_conv2 = Conv1D(1024, 1, activation='relu')(encoder_repeat1)
        encoder_conv2 = Conv1D(1024, 1, activation='relu')(encoder_conv2)
        encoder = Conv1D(1024, 1, activation='tanh')(encoder_conv2)
        encoder = Conv1D(139, 1, activation='tanh')(encoder)
        
        # LSTM Layers
        encoder_lstm = LSTM(output_dim, return_state=True, return_sequences=True, dropout=0.2, recurrent_dropout=0.1)
        encoder_outputs, state_h, state_c = encoder_lstm(encoder)
        encoder_states = [state_h, state_c]

        # Decoder Layers
        decoder_lstm = LSTM(output_dim, return_sequences=True, return_state=True, dropout=0.2, recurrent_dropout=0.1)
        decoder_outputs, _, _ = decoder_lstm(encoder_outputs, initial_state=encoder_states)
        
        # TimeDistributed Dense Layers for Output
        decoder_outputs = TimeDistributed(Dense(dense_units, activation='tanh'))(decoder_outputs)
        decoder_outputs = TimeDistributed(Dense(dense_units, activation='tanh'))(decoder_outputs)
        decoder_outputs = TimeDistributed(Dense(output_dim, activation='tanh'))(decoder_outputs)

        model = Model(encoder_inputs, decoder_outputs)
        
        # Saving model summary
        with open('model_summary.txt', 'w') as f:
            with redirect_stdout(f):
                model.summary()
        
        return model

    def train(self, input_data, target, valid_input, valid_target, batch_size=100, epochs=70):
        self.model.compile(optimizer=RAdam(), loss='mean_absolute_percentage_error')
        
        checkpointer = ModelCheckpoint(filepath="savedmodels/checkpoint/model.h5", verbose=0, save_best_only=True)
        tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)
        
        history = self.model.fit(
            input_data,
            target,
            batch_size=batch_size,
            epochs=epochs,
            shuffle=True,
            validation_data=(valid_input, valid_target),
            callbacks=[checkpointer, tensorboard]
        )
        
        return history.history

    def predict(self, input_data):
        """Predict output signal with input_data"""
        return self.model.predict(input_data)
       
    def save_model(self, model_name):
        """Save model"""
        self.model.save_weights(model_name)

    def load_model(self, model_name):
        """Load model"""
        self.model.load_weights(model_name)
